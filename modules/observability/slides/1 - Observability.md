# Observability

## Introduction

While decoupled services are easy to scale and manage, increasing interactions between those services have created a new set of problems. It’s no surprise that debugging was listed as a major challenge in the [annual state of microservices report](https://tsh.io/blog/what-are-microservices-in-2020-key-findings-from-survey-report/).

When your systems are distributed, various things can go wrong. Even if you’ve written the perfect code, a node may fail, a connection may timeout, or participant servers may act arbitrarily. **The bottom line is that things will break**. And when they do, you want to be able to identify and fix the problem as soon as possible before it alters the entire system’s performance, or affects customers or your organization’s reputation. For this reason, we need observability to run today’s services and infrastructure.

People have varying knowledge of what observability means. For some engineers, it’s the old wine of [monitoring](https://iamondemand.com/blog/how-to-properly-monitor-your-k8s-clusters-and-pods/) in a pristine bottle. For others, it’s an umbrella concept that includes log analysis, trace analysis for distributed systems, visualization, and alerts management. Honeycomb, in its [Guide to Achieving Observability](https://www.honeycomb.io/wp-content/uploads/2018/07/Honeycomb-Guide-Achieving-Observability-v1.pdf), defines observability as the ability to ask arbitrary questions about your production environment without having to know beforehand what you wanted to ask. 

Despite the variability in these definitions, they all explain **the overarching goal of observability, which is to achieve better, unprecedented visibility into systems. Observability is a property enabling you to understand what’s happening inside your software, from the outside.** An observable system provides all the information you need in real time to address the day-to-day questions you might have about a system. It also enables you to navigate from effect to cause whenever the system develops a fault.

An effective observability solution may address questions like:

* Why is “y” broken?
* What went wrong during the release of feature “x”?
* Why has system performance degraded over the past few months?
* What did my service look like at point “y”?
* Is this system issue affecting specific users or all of them?

The inherent integrations and nature of distributed systems lead to *layers of distinct ownership* which are sometimes challenging to manage. **By implementing observability across a development environment, you might be able to understand your system’s failure modes and trace issues to their root cause.**

## Pillars of Observability

Let’s review a few core concepts:
* **System**: Short for system under observation (SUO). This is the cloud-native platform (and applications running on it) you care about and are responsible for.
* **Signals**: Information observable from the outside of a system. There are different signal types (the most common are logs, metrics, and traces), and they are generated by sources. 
* **Sources**: Part of the infrastructure and application layer, such as a microservice, a device, a database, a message queue, or the operating system. They typically must be instrumented to emit signals. 
* **Agents**: Responsible for signal collection, processing, and routing. 
* **Destinations**: Where you consume signals, for different reasons and use cases. These include visualizations (e.g., dashboards), alerting, long-term storage (for regulatory purposes), and analytics (finding new usages for an app). 
* **Telemetry**: The process of collecting signals from sources, routing or preprocessing via agents, and ingestion to destinations.

![](images/observability-overview.png)


## Signals
Observability is a critical concept in modern distributed systems, particularly in microservices architectures, as it enables teams to understand and troubleshoot systems effectively. The three pillars of observability—**metrics**, **logs**, and **traces**—play a vital role in providing insights into the system’s behavior and performance. 

![](images/primary-signals.png)

### Metrics
- **Definition**: Metrics are numerical values that capture key performance indicators (KPIs) about your system over time. They are typically aggregated and provide an overview of system health and performance.
- **Characteristics**:
    - **Quantitative Data**: Metrics are quantitative and can be counted or measured (e.g., CPU usage, memory consumption, request latency).
    - **Low Cardinality**: Metrics tend to have low cardinality, meaning they track fewer distinct values compared to logs or traces.
    - **Time-Series Nature**: Metrics are usually collected as time-series data and plotted on dashboards to show trends over time.
- **Types**:
    - **System Metrics**: CPU, memory, disk, and network usage.
    - **Application Metrics**: Request count, error rate, request latency, and throughput.
- **Use Cases**:
    - **Monitoring**: Continuous monitoring of performance indicators like latency, error rates, and resource consumption.
    - **Alerting**: Alerts can be set when metrics exceed predefined thresholds (e.g., high CPU usage or request errors).
    - **Capacity Planning**: Helps in predicting future resource needs based on trends in usage patterns.


| Metric name       | Label key | Label value | Label key   | Label Value | Value at time t0 | .. at t1 |
|-------------------|-----------|-------------|-------------|-------------|------------------|----------|
| heap-memory-bytes | host      | host123     | data-center | c1          | 11231            | 11200    |
| heap-memory-bytes | host      | host234     | data-center | c1          | 300203           | 412103   |

> Table 1 shows two timeseries for one example metric. Their metric names, labels and values for certain timestamps are represented in a tabular view with columns.

#### Cardinality Issue

**Cardinality** refers to the number of unique combinations of label values (or dimensions) associated with a particular metric. Each unique combination creates a new "time series" that must be stored and tracked over time.

Consider a metric that tracks the latency of API requests. It might have the following labels (dimensions):
- `status_code`: HTTP status code of the request (e.g., 200, 404, 500).
- `endpoint`: The specific API endpoint being accessed (e.g., `/users`, `/orders`).
- `region`: The geographical location of the server (e.g., `us-east-1`, `eu-west-1`).

For each unique combination of these labels, a new time series is created:
- `latency{status_code="200", endpoint="/users", region="us-east-1"}`
- `latency{status_code="404", endpoint="/users", region="us-east-1"}`

If each label can take on many unique values, the number of possible combinations (and hence the number of time series) increases exponentially.



### Logs
- **Definition**: Logs are detailed, unstructured or semi-structured textual records that describe events that occurred in the system. Logs capture the full context of operations and are the most granular observability data.
- **Characteristics**:
    - **Rich Detail**: Logs provide detailed information about the system’s state and operations, often including error messages, stack traces, and debug information.
    - **High Cardinality**: Logs can capture many unique details such as user IDs, IP addresses, and session IDs.
    - **Time-Stamped Events**: Each log entry is typically associated with a timestamp, allowing you to track events in chronological order.
- **Types**:
    - **Application Logs**: Generated by the application itself to capture business logic events, errors, or warnings.
    - **System Logs**: Logs related to the operating system, server errors, or hardware issues.
    - **Security Logs**: Logs capturing access events, login attempts, and authorization failures.
- **Use Cases**:
    - **Debugging**: Logs are essential for troubleshooting errors and understanding the state of an application at specific points in time.
    - **Auditing**: Logs can be used to track access, changes, and interactions within the system for compliance and security purposes.
    - **Incident Investigation**: In the case of system failure or unexpected behavior, logs provide the information needed to reconstruct the chain of events leading to the issue.

### Traces
- **Definition**: Traces track the path of a request as it moves through various services in a distributed system. They help visualize and analyze how requests propagate across different components.
- **Characteristics**:
    - **Distributed Context**: Traces are particularly valuable in microservices architectures where a request can span multiple services, databases, and external APIs.
    - **Span and Trace IDs**: Traces are composed of spans, which represent a single operation within a service. Each span contains a unique ID, and all spans related to a single request share the same trace ID, allowing you to track the request end-to-end.
    - **End-to-End Latency**: Traces provide visibility into the time taken by each service involved in processing a request.
- **Types**:
    - **Root Span**: The first span in a trace, usually representing the entry point of a request into the system.
    - **Child Spans**: Represent operations in downstream services, each tracked independently but associated with the same trace ID.
- **Use Cases**:
    - **Performance Optimization**: Traces help identify bottlenecks by showing how long each service or component takes to process a request.
    - **Root Cause Analysis**: Tracing allows you to pinpoint which service or component is causing slowdowns or errors.
    - **Dependency Visualization**: Traces give a clear picture of how services interact, making it easier to understand complex dependencies in a microservice architecture.

![](images/tracing.png)

### Combining Metrics, Logs, and Traces
Although each pillar serves a different purpose, they complement one another to provide full visibility into system health:
- **Metrics** provide the "what" (e.g., high CPU usage or slow response times).
- **Logs** offer the "why" by showing the details of events leading up to or during an issue.
- **Traces** offer the "how" by revealing the flow of requests through different parts of the system, making it possible to locate performance bottlenecks or failures in distributed environments.

By integrating these pillars using observability tools (e.g., Prometheus for metrics, ELK stack for logs, Jaeger for tracing), teams can achieve comprehensive insight into the behavior of their systems, which is essential for effective monitoring, debugging, and optimizing modern software systems.

## Tools and technologies

It’s important to understand the many options available when choosing an observability tool so you can select the one that best meets your organization’s needs.

**Log management tools**

[Log management](https://www.crowdstrike.com/cybersecurity-101/observability/log-management/) tools are useful for gathering and storing log data. Some solutions allow users to inspect logs in real time and create alerts for abnormalities. Log management tools are helpful for organizations that need an effective way to adhere to logging compliance requirements because they provide a quick and efficient way to gather and store information.

**Application performance monitoring (APM) tools**

APM tools monitor software applications and track the transaction speeds of end users, systems, and network infrastructure to identify performance issues or bottlenecks that may have a negative impact on the user experience. These technologies measure production application performance, allowing users to discover problems and determine their root cause.

**Observability platforms**

Unlike individual tools, observability platforms (see an example [here](https://www.datadoghq.com/dg/enterprise/log-management-analytics-security/)) provide organizations with insights and continuous feedback from their systems. A single observability platform that includes all three capabilities — monitoring, logging, and tracing — can deliver a comprehensive picture of the state of an organization’s systems and services from across their infrastructure. With an observability platform analyzing a company’s centralized telemetry data, it increases the data’s value and delivers meaningful context for teams to make business-critical decisions across use cases.

## Best practices for implementing observability

With an understanding of observability and the value it provides, you can use the list below to implement a perfect solution for your organization. In short, your observability platform should be able to:

* Integrate with all of your systems across each of your application stacks, either natively or through plugins
* Install in an automated, reproducible way
* Capture real-time data from all target components and store, index, and correlate them in a meaningful and cost-effective way
* Provide an overall picture of your complex system in real time
* Support traceability to show exactly where something is going wrong and do this by separating the important information from the noise
* Provide historical trends and anomaly reports
* Show all relevant, contextual data in alerts and reports
* Offer a user-friendly interface while supporting the creation of customized, aggregated reports for different teams


## Resources
